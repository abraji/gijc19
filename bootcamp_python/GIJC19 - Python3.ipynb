{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "# Python 3 - Raspar dados não estruturados com BeautifulSoup\n",
    "\n",
    "<h4>Por <a href=\"https://www.twitter.com/tbkaas\">Tommy Kaas</a> e <a href=\"https://www.twitter.com/nmulvad\">Nils Mulvad</a>, Kaas & Mulvad</h4>\n",
    "\n",
    "Este notebook acompanha a sessão \"Python 3\" para recuperar **dados não estruturados** da Internet.\n",
    "\n",
    "Na última sessão, usamos a biblioteca `requests` para recuperar **dados estruturados** da internet. Também tentamos engenharia reversa de sites para encontrar uma API.\n",
    "\n",
    "Trabalhamos com o  **JSON** formato, que extraímos do nosso `response` objeto usando o `response.json()` método.\n",
    "\n",
    "Porém, quando chamamos URLs que nos deram **JSON**, agora recuperaremos dados para um site sem essas opções e usaremos uma nova biblioteca para encontrar e extrair os snippets necessários, na grande caixa bagunçada de HTML e CSS .\n",
    "\n",
    "---\n",
    "\n",
    "Entra o `BeautifulSoup`.\n",
    "![](images/unstructured_data.jpg)\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "\n",
    " \n",
    "---\n",
    "\n",
    "![](images\\soup_doc.jpg)\n",
    "\n",
    "---\n",
    "\n",
    "A documentação é encontrada aqui: `https://www.crummy.com/software/BeautifulSoup/bs4/doc/`.\n",
    "\n",
    "O primeiro passo é importar a ferramenta. `BeautifulSoup` faz parte de um pacote chamado `bs4`. Uma vez que não precisamos de todo `bs4`, mas apenas `BeautifulSoup`, nós apenas importamos esta parte.\n",
    "\n",
    "Isto se faz do seguinte modo:\n",
    "\n",
    "`from bs4 import BeautifulSoup`\n",
    "\n",
    "Experimente abaixo - e importe `requests` também."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importe bs4 aqui - e importe também requests. Vamos precisar dos dois ...\n",
    "import requests\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "O primeiro passo é abrir um site e obter dados para o BeautifulSoup.\n",
    "Neste exemplo, usaremos um site com informações sobre jogadoras de futebol. Mais específico: queremos raspar 12 páginas com informações de todos os jogadoras da Copa do Mundo Feminina de 2019. Os dados incluem nome, país, data de nascimento, altura e posição.\n",
    "\n",
    "A primeira das 12 páginas está aqui:\n",
    "https://www.worldfootball.net/players_list/frauen-wm-2019-frankreich/nach-mannschaft/1/\n",
    "\n",
    "Você deve abrir e verificar. É sempre uma boa idéia ser minucioso ao pesquisar o site e as páginas que você deseja raspar.\n",
    "\n",
    "Vamos obter dados da primeira página. Vamos usar um verdadeiro `\"User-Agent\"`, porque não vamos gritar muito alto, que somos robôs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# defina uma variável para o URL. Você receberá uma ajudinha agora ...\n",
    "url = \"https://www.worldfootball.net/players_list/frauen-wm-2019-frankreich/nach-mannschaft/1/\"\n",
    "\n",
    "# crie um header\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'\n",
    "}\n",
    "\n",
    "# retrive data to an object called r. Don't forget the header.\n",
    "r = requests.get(url, headers=header)\n",
    "\n",
    "# extraímos o código fonte da página em um objeto chamado html.\n",
    "html = r.text\n",
    "\n",
    "# agora vamos enviar o código fonte através do BeautifulSoup e carregar o produto em um objeto chamado soup\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#vamos imprimi-lo.\n",
    "#print(soup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A primeira parte dos scripts é apenas uma repetição do que fizemos na última sessão (obtendo dados estruturados).\n",
    "\n",
    "Nós então usamos `.json()` para processar a resposta que recebemos do servidor, mas agora (linha 13 no script acima) usamos `.text`.\n",
    "\n",
    "O motivo é que o servidor não retorna  **JSON**-data, mas **HTML**. Se tentássemos usar `.json()`, receberíamos um erro (uma *exceção*).\n",
    "\n",
    "Basicamente, você pode dizer que `r.text` é usado se precisarmos obter o que corresponde ao código-fonte de um site (ou seja, pressionar CTRL + U no Chrome), ou seja, o HTML puro.\n",
    "\n",
    "Na linha 16, carregamos nosso *código-fonte*, html, no `BeautifulSoup`. É habitual entre usuários de `BeautifulSoup` chamar o objeto no qual o HTML está carregado de `soup` - assim como, por exemplo, também está entre os usuários de `requests` chamar a resposta do servidor de `r`. Então, faremos isso aqui também.\n",
    "\n",
    "Novo também é o segundo argumento em `BeautifulSoup`: `\"html.parser\"`.\n",
    "\n",
    "Este argumento diz `BeautifulSoup` qual *analisador* (parser) precisa rastrear o HTML e, entre outras coisas, corrigir erros e omissões.\n",
    "\n",
    "`html.parser` está embutido no Python 3. (O Python vem com *baterias incluídas*). Mas também existem outros analisadores, incluindo um chamado` lxml`, que é mais rápido e mais adequado para, por exemplo, analisar dados **XML**.\n",
    "\n",
    "Contudo, `lxml` não vem com o pacote desde o início e é um programa separado que historicamente tem sido um pouco difícil de instalar no Windows.\n",
    "\n",
    "Portanto, neste processo, usamos o que acompanha o Python, `\"html.parser\"`.\n",
    "\n",
    "No exemplo acima, imprimimos nosso objeto de soup e ele fornece muito HTML, mas pode ser um pouco difícil de ler. Assim como você pode usar `pprint` no **JSON**, existe algo semelhante para `BeautifulSoup`, uma função chamada `.prettify()`, que usamos no objeto bs4 que queremos tornar mais legíveis, como`soup.prettify()`.\n",
    "\n",
    "Execute a célula abaixo e veja a diferença."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# vamos fazer a sopa mais legível\n",
    "print(soup.prettify())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TPara extrair bits da * sopa *, você precisa conhecer um pouco sobre HTML e CSS.\n",
    "\n",
    "HTML consiste em tags, agrupadas nos símbolos `<` and `>`. As tags comuns são:\n",
    "\n",
    "- `a` - a link (a é para **a**nchor - âncora)\n",
    "- `title` - título da página\n",
    "- `img` - uma imagem\n",
    "- `br` - nova linha\n",
    "- `table` - a tabela\n",
    "- `tr` - uma linha em uma tabela (tr significa linha da tabela)\n",
    "- `td` - é usado para definir uma célula padrão na tabela. Significa divisão de tabelas\n",
    "\n",
    "___\n",
    "\n",
    "## BeautifulSoup and HTML\n",
    "\n",
    "Vamos nos concentrar no puro `HTML` nesta sessão.\n",
    "\n",
    "Quando pedimos `BeautifulSoup` para procurar uma determinada tag, a ferramenta começa na parte superior do nosso HTML e trabalha até o final.\n",
    "\n",
    "`BeautifulSoup` tem em seu núcleo duas maneiras de procurar o que estamos procurando.\n",
    "- `.find()` - *retorna o primeiro elemento que conclui a pesquisa (e Nenhum, se não houver nenhum)*\n",
    "- `.find_all()` - *retorna uma lista de todos os itens que correspondem à pesquisa (e uma lista vazia, se não houver nenhum)*\n",
    "\n",
    "Portanto, a pergunta que devemos fazer é tipicamente:\n",
    "\n",
    "- *** Estou procurando uma tag ou estou procurando muitas? ***\n",
    "\n",
    "![](images/find_vs_find_all.jpg)\n",
    "\n",
    "___\n",
    "\n",
    "# .find()\n",
    "\n",
    "---\n",
    "\n",
    "Vamos começar procurando por um elemento do nossa `soup` do site de futebol. Vamos usar a função `.find() `.\n",
    "\n",
    "Para fins de prática, primeiro selecionamos a `HTML` tag `title`, que mostra o título da página.\n",
    "\n",
    "**Escreva o seguinte** duas linhas de código no campo abaixo. Execute o código e localize a tag title:\n",
    "\n",
    "title_tag = soup.find('title')\n",
    "\n",
    "print(title_tag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como regra, não estamos interessados no `HTML` em si, mas o que está dentro dele. Podemos extrair isso usando o `.get_text()` método no nosso `title_tag`.\n",
    "\n",
    "Vamos tentar isso.\n",
    "\n",
    "**Escreva o seguinte** no campo abaixo e execute novamente:\n",
    "\n",
    "title = title_tag.get_text()\n",
    "\n",
    "print(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Os títulos HTML vêm em vários tamanhos.\n",
    "\n",
    "O maior é chamado `h1`, o próximo` h2`, então `h3`, etc.\n",
    "\n",
    "**Vamos tentar encontrar mais algumas coisas na soup:**\n",
    "\n",
    "---\n",
    "\n",
    "- No campo abaixo, extraia um `h4` tag da nossa `soup` com `.find ()`\n",
    "- Imprimir apenas o conteúdo da tag (ou seja, sem tags)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .find() para procurar uma 'h4'-tag\n",
    "\n",
    "# extraia e imprimima o que está dentro da tag com .get_text()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Muitas vezes vamos usar `.find()` em um elemento, descobrimos usando `find()`. \n",
    "Pode ser, por exemplo, se extrairmos uma tabela (`<table>`) do html para um elemento independente e, nesta nova seção, deseja encontrar outro elemento, por exemplo `<tr>`, ou seja, uma linha em uma tabela.\n",
    "\n",
    "Normalmente, fazemos assim:\n",
    "\n",
    "```python\n",
    "table = soup.find('table')\n",
    "tr = table.find('tr')\n",
    "\n",
    "```\n",
    "\n",
    "Dessa forma, é possível cavar cada vez mais uma estrutura HTML - de fato, bastante semelhante à que fizemos ao trabalhar com o **JSON**.\n",
    "\n",
    "---\n",
    "\n",
    "![](images/find_all.png)\n",
    "\n",
    "---\n",
    "\n",
    "# .find_all()\n",
    "\n",
    "---\n",
    "\n",
    "Está bem. Agora sabemos, podemos usar `.find ()` para encontrar uma única tag.\n",
    "\n",
    "Agora vamos tentar procurar *muitas* tags de algum tipo. Vamos testar a função `.find_all()` , onde encontramos todas as tags de um determinado tipo. Ao contrário de `.find ()`, essa função sempre retorna uma lista - mesmo se houver apenas uma ocorrência ou nenhuma ocorrência.\n",
    "\n",
    "Podemos pedir ao BeautifulSoup para procurar mais de um tipo de tag, se colocarmos as tags dentro de uma lista.\n",
    "\n",
    "Então aqui queremos encontrar tudo de <td> ou <th> tags: `.find_all(['th','td'])`\n",
    "    \n",
    "\n",
    "***Tente isso:***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# quantas 'h4'-tag você pode encontrar na soup? - dica: use find_all() ao invés de find(). E conte com len()\n",
    "\n",
    "# quantos links você pode encontrar na soup? dica: procure por 'a' tag\n",
    "\n",
    "# quantos 'table'-tags você pode encontrar na soup\n",
    "\n",
    "# em qual das tabelas encontramos os nomes das 50 jogadorea?\n",
    "\n",
    "# imprime o nome da primeira jogadora\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Agora - vamos combinar o que fizemos. Queremos escrever um script, que abra a página, recupere os dados e, no html, encontre todos os nomes dos jogadores e os imprima.\n",
    "\n",
    "Você pode obtê-lo em duas versões: o código puro sem comentários e um com comentários linha por linha."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#script sem comentários:\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "url = \"https://www.worldfootball.net/players_list/frauen-wm-2019-frankreich/nach-mannschaft/1/\"\n",
    "\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'\n",
    "}\n",
    "\n",
    "r = requests.get(url, headers=header)\n",
    "html = r.content.decode('utf-8') \n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "with open(\"playernames.txt\", 'w', encoding='utf-8') as f:\n",
    "    table = soup.find_all('table')\n",
    "    player_table = table[1]\n",
    "    rows = player_table.find_all('tr')\n",
    "    for row in rows:\n",
    "        cols = row.find_all(['th','td'])\n",
    "        player_name = cols[0].get_text()\n",
    "        country = cols[2].get_text()\n",
    "        born = cols[3].get_text()\n",
    "        height = cols[4].get_text()\n",
    "        position = cols[5].get_text()\n",
    "        f.write(\"{};{};{};{};{}\\r\\n\".format(player_name, country, born, height, position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#script com comentários completos para cada etapa:\n",
    "\n",
    "#nós importamos as bibliotecas necessárias\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "#nós definimos a variável url\n",
    "url = \"https://www.worldfootball.net/players_list/frauen-wm-2019-frankreich/nach-mannschaft/1/\"\n",
    "\n",
    "#nós escrevemos um cabeçalho e definimos uma variável\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'\n",
    "}\n",
    "\n",
    "#visitamos a página com nosso cabeçalho definido e obtemos o conteúdo - e adicionamos isso à variável r\n",
    "r = requests.get(url, headers=header)\n",
    "\n",
    "#normalmente \"html = r.text\" seria bom, mas nesse caso, precisamos especificar a codificação para evitar erros\n",
    "html = r.content.decode('utf-8') \n",
    "\n",
    "#executamos o html no BeautifulSoup e usamos o html-parser\n",
    "soup = BeautifulSoup(html, \"html.parser\")\n",
    "\n",
    "#agora definimos um arquivo de texto gravável. Nós chamamos de playernames\n",
    "with open(\"playernames.txt\", 'w', encoding='utf-8') as f:\n",
    "    \n",
    "    #extraímos todas as tabelas para a tabela variável\n",
    "    table = soup.find_all('table')\n",
    "    \n",
    "    #extraímos a tabela com o número de índice 1 e a adicionamos à variável player_table\n",
    "    player_table = table[1]\n",
    "    \n",
    "    #de player_table, encontramos todas as linhas e as adicionamos às linhas variáveis\n",
    "    rows = player_table.find_all('tr')\n",
    "    \n",
    "    #com um \"for loop\", iteramos pelas linhas.\n",
    "    for row in rows:\n",
    "        \n",
    "        #em cada linha, encontramos todas as tags td (usadas para separar os campos da tabela em cada linha)\n",
    "        cols = row.find_all(['th','td'])\n",
    "        \n",
    "        #o find_all ('td') resulta em uma lista. Para especificar qual tag td, precisamos, usamos números de índice.\n",
    "        \n",
    "        #Retiramos o texto da primeira tag td e o adicionamos à variável player_name\n",
    "        player_name = cols[0].get_text()\n",
    "        \n",
    "        #Retiramos o texto da terceira tag td e o adicionamos à variável country\n",
    "        country = cols[2].get_text()\n",
    "        \n",
    "        #Retiramos o texto da quarta tag td e o adicionamos à variável born\n",
    "        born = cols[3].get_text()\n",
    "        \n",
    "        #Retiramos o texto da quinta tag td e o adicionamos à variável height\n",
    "        height = cols[4].get_text()\n",
    "        \n",
    "        #Retiramos o texto da sexta tag td e o adicionamos à variável position\n",
    "        position = cols[5].get_text()\n",
    "        \n",
    "        #Escrevemos uma linha no arquivo de texto. \\r\\n garante que separamos as linhas corretamente.\n",
    "        f.write(\"{};{};{};{};{}\\r\\n\".format(player_name, country, born, height, position))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "E finalmente: raspamos a primeira página, mas gostaríamos de raspar todas as 12 páginas.\n",
    "\n",
    "Nós usamos um `for loop` e criamos os números de 1 a 12 (para inserir no URL) com um `range(1,13)`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nós importamos as bibliotecas necessárias\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "header = {\n",
    "    'User-Agent': 'Mozilla/5.0 (Windows NT 6.1; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/63.0.3239.84 Safari/537.36'\n",
    "}\n",
    "\n",
    "with open(\"playernames.txt\", 'w', encoding='utf-8') as f:\n",
    "    for n in range(1,13):\n",
    "        url = \"https://www.worldfootball.net/players_list/frauen-wm-2019-frankreich/nach-mannschaft/{}/\".format(n)\n",
    "        r = requests.get(url, headers=header)\n",
    "        html = r.content.decode('utf-8') \n",
    "        soup = BeautifulSoup(html, \"html.parser\")\n",
    "        table = soup.find_all('table')\n",
    "        player_table = table[1]\n",
    "        rows = player_table.find_all('tr')\n",
    "        for row in rows[1:]:\n",
    "            cols = row.find_all('td')\n",
    "            player_name = cols[0].get_text()\n",
    "            country = cols[2].get_text()\n",
    "            born = cols[3].get_text()\n",
    "            height = cols[4].get_text()\n",
    "            position = cols[5].get_text()\n",
    "            f.write(\"{};{};{};{};{}\\r\\n\".format(player_name, country, born, height, position))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Onde aprender mais\n",
    "\n",
    "**Tutoriais etc.**\n",
    "\n",
    "Esperamos que essas três sessões tenham um entendimento básico, mas há muito mais a explorar.\n",
    "\n",
    "Existem muitos tutoriais na web. Alguns são extremamente bons. Se você tentar um e não gostar ou conseguir, tente outro. Mas os livros também podem ser ótimos.\n",
    "Aprendi muito com Magnus Lie Hetlands, “Python from Beginning Python: From Novice to Professional”.\n",
    "https://www.amazon.com/Beginning-Python-Professional-Edition-Professionals/dp/1590599829\n",
    "\n",
    "Este é o tutorial oficial do Python: https://docs.python.org/2/tutorial/\n",
    "\n",
    "No Python Wiki: Uma longa lista de tutoriais para não programadores: https://wiki.python.org/moin/BeginnersGuide/NonProgrammers\n",
    "\n",
    "No Youtube e em muitos outros sites, você encontrará screencasts / gravações de vídeo de programação. Feito da maneira correta, os tutoriais em vídeo podem ser excelentes.\n",
    "\n",
    "\n",
    "**Se você empacar**\n",
    "\n",
    "Google para a resposta. Pergunte a um amigo ou grupo de usuários. Leia Como fazer perguntas, antes de postar suas perguntas online. Ele abrange maneiras de ajudar a enquadrar suas perguntas para que outras pessoas possam ajudá-lo da melhor maneira.\n",
    "https://www.propublica.org/nerds/item/how-to-ask-programming-questions\n",
    "\n",
    "O Python.org possui listas de grupos de notícias e listas de discussão para iniciantes em Python:\n",
    "http://mail.python.org/mailman/listinfo\n",
    "\n",
    "Uma das listas de discussão \"Tutor\" é descrita assim: \"Esta lista é para pessoas que desejam fazer perguntas sobre como aprender programação de computadores com a linguagem Python\".\n",
    "\n",
    "Em resumo, pode-se enviar sua pergunta para a lista, e em breve haverá respostas e essas respostas geralmente são muito competentes. Eles não vão te dar a solução final. Você ainda quer aprender a codificar, certo?\n",
    "\n",
    "Um grupo de jornalistas que criaram criou a lista de discussão PythonJournos. Funciona da mesma maneira - se você tiver um problema (em relação ao Python), envie-o para a lista de emails e conte com respostas boas e rápidas. No momento (setembro de 19), o grupo tem 498 membros e está aberto a todos.\n",
    "\n",
    "https://github.com/PythonJournos/LearningPython/wiki\n",
    "https://groups.google.com/forum/?hl=en#!forum/PythonJournos\n",
    "\n",
    "Journalism.co.uk: 4 maneiras pelas quais os jornalistas podem obter ajuda com seu código:\n",
    "https://www.journalism.co.uk/news/4-ways-journalists-can-get-help-with-code/s2/a570167/\n",
    "\n",
    "\n",
    "Contact: \n",
    "\n",
    "**Tommy Kaas** Twitter: @tbkaas mail: tommy.kaas@kaasogmulvad.dk\n",
    "\n",
    "**Nils Mulvad** Twitter: @nmulvad mail: nils.mulvad@kaasogmulvad.dk\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nota da tradução: No Brasil a Abraji e a Escola de Dados também possuem um fórum para dúvidas e debates sobre Python e outras linguagens: \n",
    "        \n",
    "https://forum.jornalismodedados.org/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
